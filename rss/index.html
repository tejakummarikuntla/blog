<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[tejakummarikuntla]]></title><description><![CDATA[Learn | Innovate | Deliver]]></description><link>tejakummarikuntla.github.io/blog/</link><image><url>tejakummarikuntla.github.io/blog/favicon.png</url><title>tejakummarikuntla</title><link>tejakummarikuntla.github.io/blog/</link></image><generator>Ghost 3.13</generator><lastBuildDate>Mon, 04 May 2020 18:10:40 GMT</lastBuildDate><atom:link href="tejakummarikuntla.github.io/blog/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Snorkeling in Data for Supervision and Generation]]></title><description><![CDATA[<h2 id="let-s-start-with-why-"><strong>Let's Start with 'Why?'</strong></h2><p>Most of the preprocessing phase of pipelines has the complexity of taking the unstructured data or the Dark Data like text, Tables, Image, etc, and turning into the structured data which usually takes months or years and to build ML models further.Before building an</p>]]></description><link>tejakummarikuntla.github.io/blog/snorkeling-in-data-for-supervision-and-generation/</link><guid isPermaLink="false">5ea5ec98f432ef475c61ed01</guid><category><![CDATA[Data Science]]></category><dc:creator><![CDATA[Teja Kummarikuntla]]></dc:creator><pubDate>Sun, 26 Apr 2020 20:20:58 GMT</pubDate><media:content url="tejakummarikuntla.github.io/blog/content/images/2020/04/Snorkel_HighLevel_WorkFlow-1.png" medium="image"/><content:encoded><![CDATA[<h2 id="let-s-start-with-why-"><strong>Let's Start with 'Why?'</strong></h2><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/Snorkel_HighLevel_WorkFlow-1.png" alt="Snorkeling in Data for Supervision and Generation"><p>Most of the preprocessing phase of pipelines has the complexity of taking the unstructured data or the Dark Data like text, Tables, Image, etc, and turning into the structured data which usually takes months or years and to build ML models further.Before building an ML model over the extracted data, we need to Hand label it, those are called Gold Labels. If we look from Today's Machine Learning pipeline at a high level this explicitly says that people spent most of the time on creating the Training Dataset and little hustle over Feature Engineering as Deep Learning makes it easy, the crucial part of creating a Training data set is labeling that data points correctly, because the performance of the end model totally depends on how well it trained with correct labels. Now, the question is can we hasten up the process of labeling using any Framework ?, This is where Snorkel began to accelerate Data Building and Managing, not only labeling, snorkel has many other features that make the bottlenecks unclogged.Snorkel: A framework for Rapidly Generating Training Data with Weak Supervision.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/Screen-Shot-2020-01-23-at-9.03.38-PM.png" class="kg-image" alt="Snorkeling in Data for Supervision and Generation"><figcaption>Snorkel over Data Creation and Feature Engineering.</figcaption></figure><h2 id="weak-supervision"><strong>Weak Supervision</strong></h2><p>By eliminating the Hand labeling process, Now we can programmatically generate labels with external Domain knowledge or any patterns. This results in low-quality lables[Weak labels] more efficiently, which means Weak labels are intended to decrease the cost and increase efficiency. Using noisy, imprecise sources for building a large amount of training data in supervised learning is called Weak Supervision. One of the famous methodologies of weak labeling is Distant supervision. To reiterate Snorkel is an open-source system for quickly assembling training data through weak supervision.</p><h2 id="what-snorkel-can-do"><strong>What Snorkel can do?</strong></h2><p>Snorkel currently has three features for creating and handling training data sets.</p><ul><li><strong><strong>Data Labeling:</strong></strong> Assigning a value to each data point based on Heruristc, Distant Supervision Techniques, etc.,</li><li><strong><strong>Data Transformation:</strong></strong> Converting existing data from one format to another or modifying the data which doesn't affect actual labels. e.g: rotating an image in different angles, etc.,</li><li><strong><strong>Data Slicing:</strong></strong> Segmenting a data set into required subsets for different purposes like improving model perfomance.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/fig_abstractions-1.png" class="kg-image" alt="Snorkeling in Data for Supervision and Generation"><figcaption>Source: snorkel.org</figcaption></figure><h2 id="how-snorkel-does-it"><strong>How Snorkel does it?</strong></h2><p>The Hight level Architecture of Snorkel's Workflow consist of Five Steps:</p><ol><li>Writing Labeling Functions. (LFs)</li><li>Modeling and Combining Labeling Functions.</li><li>Writing Transformation Functions for perfoming Data Augmentation.</li><li>Writing Slicing Functions for Subset selection.</li><li>Training a final ML Model.</li></ol><p>Following up these, the takeaway parts of Snorkel is its ability to use labels from different Weak Supervision sources and the set of all labeling functions are modeled by combining and applying a Generative Model [LabelModel] that generates a weak set of labels. Along with that, the system can output Probablistic labels that can be used to train various Classifiers, which indeed generalizes noise labels.</p><figure class="kg-card kg-image-card kg-width-full"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/Snorkel_HighLevel_WorkFlow.png" class="kg-image" alt="Snorkeling in Data for Supervision and Generation"></figure><h2 id="data-labeling"><strong>Data Labeling</strong></h2><p>There are few diffent ways that you would totally turn into weak suprevision by using a python decorator <code>labeling_function()</code> or a python class <code>LabelingFunction</code>.</p><ul><li><strong><strong>Heuristics</strong></strong>: applying set of conditions by a pattern, ex: using regular expressions</li><li><strong><strong>Third party Models</strong></strong>: Using an exisiting Model to perfome labeling.</li><li><strong><strong>Distant Supervision</strong></strong>: Using Existing ground truth data that imprecisely fits in, [External Knowledge Bases].</li></ul><p>Let's suppose, we have three different labeling functios are written using conditional pattern and Regular expresions. Expample taken form Snorkel-tutorials.</p><pre><code>@labeling_function()
def check(x):
    return SPAM if "check" in x.text.lower() else ABSTAIN

@labeling_function()
def check_out(x):
    return SPAM if "check out" in x.text.lower() else ABSTAIN
  
# with Regex
@labeling_function()
def regex_check_out(x):
    return SPAM if re.search(r"check.*out", x.text, flags=re.I) else ABSTAIN</code></pre><p>Now, these labeling functions can label in their own way, which is completely independent, the lables would varry.</p><h2 id="applying-lfs"><strong>Applying LFs</strong></h2><p>Snorkel provides Labeling Functions applier for Pandas DataFrames, we can use <code>PandasLFApplier(lfs)</code> which takes a list of labeling functions and return <code>Label Matrix</code> in which each columns represents the outputs of each labeling function in the input list.</p><pre><code>lfs = [check_out, check, regex_check_out]

applier = PandasLFApplier(lfs=lfs)
L_train = applier.apply(df=df_train)</code></pre><figure class="kg-card kg-image-card"><img src="tejakummarikuntla.github.io/blog/content/images/2020/05/L_Train.png" class="kg-image" alt="Snorkeling in Data for Supervision and Generation"></figure><p>For understanding the perfomance and analysing multiple labeling functions let's burst some terminologeis.</p><ul><li><strong><strong>Polarity</strong></strong>: The set of unique labels that LF outputs</li><li><strong><strong>Coverage</strong></strong>: The fraction of the dataset the LF labels</li><li><strong><strong>Overlaps</strong></strong>: The fraction of the dataset where the one LF and atleast one other LF label same.</li><li><strong><strong>Conflicts</strong></strong>: The fraction of the dataset where one LF and at least one other LF label and disagree</li><li><strong><strong>Correct</strong></strong>: The number of data points that the LF labels correctly (if gold labels are provided)</li><li><strong><strong>Incorrect</strong></strong>: The number of data points that this LF labels incorrectly (if gold labels are provided)</li><li><strong><strong>Empirical Accuracy</strong></strong>: The empirical accuracy of the LF (if gold labels are available)</li></ul><p>When we apply LFAnalysis[Labeling Functions Analysis] utility, it results the above metrics for each labeling function.</p><pre><code class="language-python">lfs = [check, check_out, regex_check_out]
LFAnalysis(L=L_train, lfs=lfs).lf_summary(Y=Y_dev)
</code></pre><figure class="kg-card kg-image-card"><img src="tejakummarikuntla.github.io/blog/content/images/2020/05/LFAnalysis.png" class="kg-image" alt="Snorkeling in Data for Supervision and Generation"></figure><p>	After done writing Labeling Functions, and the L_Train has respective columns that represents the corrosponding outputs, our goal is to consie and ¬†generate one standard column which has noise-aware probablistic label per data that can be appended to unlabeled dataset for futher training purpose.</p><p>This process of generating the final label column can be done in few approaches. We can take a Majority vote from the L_train for each data point and result a single value.</p><pre><code class="language-python">from snorkel.labeling import MajorityLabelVoter

majority_model = MajorityLabelVoter()
preds_train = majority_model.predict(L=L_train)
</code></pre><p>One other approch is Snorkel can train a Label Model that takes advange of conflicts between all Labeling Functions and estimate their accuracy. This model with produce a singel set of noise-aware labels, which are porbablistic [Confident-Weighted]. We now can use the resulting probablistic label values to train various classifiers.</p><p>We can use techniques like Logistic Regression, SVM, LSTM at this stage. The discriminative model learn feature representation of our labelling functions and this makes it better able to generalize to unseen data. This will increase the recall and produces final output.</p><pre><code class="language-python">from snorkel.labeling import LabelModel

label_model = LabelModel(cardinality=2, verbose=True)
label_model.fit(L_train=L_train, n_epochs=500, lr=0.001, log_freq=100, seed=123)
preds_train = label_model.predict(L=L_train)
</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="tejakummarikuntla.github.io/blog/content/images/2020/05/snorkel_Labeling_overview.png" class="kg-image" alt="Snorkeling in Data for Supervision and Generation"><figcaption>Source: <a href="https://blog.fastforwardlabs.com/2019/09/27/automating-weak-supervision.html">Automating Weak Supervision</a></figcaption></figure><h2 id="data-transformation">Data Transformation</h2><p>Transformation is a technique of Data Augmentation, a proper data augmentation certainly boosts up the model performance. ¬†Computer vision is the area of work where Data augmentation is used extensively, an Image can be augmented by rotating, flipping or adding filters, etc. When it befalls to the part of Text the complexity of applying Augmentation goes up. A simple example of transforming a text is by replacing the existing words in the document by its synonyms. ¬†But not every word can be replaceable such as a, an, the, etc.</p><p>When we ask what that really makes Data Transformation a big difference is the More the data we have, the better the model performs. ¬†As we transform a data point in different ways, we certainly do not affect the label so it explicitly generates data that could most benefit the training phase.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://labs.imaginea.com/content/images/2020/01/data_agu_image.png" class="kg-image" alt="Snorkeling in Data for Supervision and Generation"><figcaption>Source: <a href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;ved=2ahUKEwiD6qW5wZrnAhVC9nMBHSsUDRwQjRx6BAgBEAQ&amp;url=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1186%2Fs40537-019-0197-0&amp;psig=AOvVaw1mwDEiBpoljY1029YvQxDz&amp;ust=1579896010264612" style="box-sizing: inherit; margin: 0px; padding: 0px; border: 0px; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; line-height: inherit; font-family: inherit; font-size: 17.6px; vertical-align: baseline; background-color: transparent; color: rgb(0, 0, 0); text-decoration: none; word-break: break-word; box-shadow: rgb(62, 176, 239) 0px -1px 0px inset;">Survey on Image Data Augmentation</a></figcaption></figure><h2 id="writing-transformation-functions">Writing Transformation Functions</h2><p>Snorkel provides a python decorator `transformation_function` which takes a single data point and returns the transformed version of it. If the data transformation isn't done it returns `None`, If all the TFs applied to a data point return None, the data point won't be included in the augmented dataset when we apply our TFs below.</p><pre><code class="language-python">@transformation_function(pre=[spacy])
def change_person(x):
    person_names = [ent.text for ent in x.doc.ents if ent.label_ == "PERSON"]
    # If there is at least one person name, replace a random one. Else return None.
    if person_names:
            name_to_replace = np.random.choice(person_names)
            replacement_name = np.random.choice(replacement_names)
            x.text = x.text.replace(name_to_replace, replacement_name)
            return x</code></pre><h3 id="applying-transformation-functions-"><strong>Applying Transformation Functions.</strong></h3><p>A ¬†little similar approach as applying labeling functions, Snorkel provides a specific class for Pandas DataFrame `PandasTFApplier`, where the PandasTFApplier takes list of transformation functions and a Policy, a Policy is used to determine what sequence of Transformation functions to apply, here we use `mean_field_policy`, which allows specifying a sampling distribution for the Transformation Functions.</p><pre><code class="language-python">from snorkel.augmentation import PandasTFApplier

tfs = [change_person, swap_adjectives, replace_verb_with_synonym, replace_noun_with_synonym, replace_adjective_with_synonym]

tf_applier = PandasTFApplier(tfs, mean_field_policy)
df_train_augmented = tf_applier.apply(df_train)
Y_train_augmented = df_train_augmented["label"].values</code></pre><h2 id="reference-"><strong>Reference:</strong></h2><p><a href="https://github.com/snorkel-team/snorkel-tutorials">Snorkel Tutorials</a>; <a href="https://labs.imaginea.com/snorkeling-in-data-for-supervision-and-generation/snorkel.org">snorkel.org</a>; <a href="https://towardsdatascience.com/snorkel-a-weak-supervision-system-a8943c9b639f">Snorkel a weak supervision systerm;</a> <a href="https://www.google.com/search?q=introducing+snorkl+medium&amp;rlz=1C5CHFA_enIN836IN836&amp;oq=introducing+snorkl+medium&amp;aqs=chrome..69i57j33.4575j1j4&amp;sourceid=chrome&amp;ie=UTF-8">Introducing Snorkel</a>; <a href="https://hazyresearch.github.io/snorkel/blog/june-2019-workshop.html">Snorkel-June-2019-workshop</a>;</p><h2 id="my-exprience-at-imaginea-labs-"><strong>My Exprience at Imaginea Labs.</strong></h2><blockquote>A short period of time may not give you much experience, but it leaves you with constructive insights that you could transform the way you admire. ¬† ¬† ¬† - tejakummarikuntla.</blockquote><p>I'm the youngest friend to everyone in the team not only in age but also in thoughts and actions ;p. I always feel that the connections and habits you build will build you back and I tried every day doing it my best. I spent most of my time to Unlearn and relearn in the other dimension that could easily create vivid impressions over the journey and this is the endgame of my Internship journey[24/01/2020].</p><h3 id="tl-dr"><strong>TL;DR</strong></h3><p>Here are a few takeaways that I could only mention by writing. As there are many (intuitions/Familirites) that I couldn't express in words.</p><blockquote>Sri would be probably chuckling when he notices that I tried using Intuition and Familiarity interchangeably [<a href="http://www.userinsight.com/familiarity-breeds-intuition/">Familiarity Breeds Intution</a>]</blockquote><ul><li>Constructive learning approach.</li><li>Learning how to learn.</li><li>Art of not negotiating peculiar advice.</li><li>Aligning the mathematical thinking with programmatical implementation.</li><li>Unlearning.</li><li>Lucid approach to understanding a research paper.</li><li>It's not about how much you know, it's all about what you can devise with it.</li><li>Tracking the new learning by logging in Latex or Markdown.[Zotero]</li><li>You can only know the curx when you keep questioning 'Why?'</li><li>Mathematical bonding not only with life but also with Music and Art [Godel, Escher, Bach]</li><li>You can only Enjoy working when you see a purpose in it.</li><li>Healthy relations will amplify performance.</li><li>When you start expressing, you will start noticing different results.</li><li>Laughing for lame jokes isn't a sin, coz I do a lot ;p [Not lame jokes ] I know it's very lame ;p.</li></ul><p>I've updated my technical progess at <a href="https://www.linkedin.com/tejakummarikuntla/imaginea_assign">GitHub</a> [Never missed a day to commit :D], feel free to check out and keep in touch @ <a href="https://www.linkedin.com/in/tejakummarikuntla">LinkedIn</a>, <a href="https://www.instagram.com/tejakummarikuntla">Instagram</a>.</p><p><strong><strong>Thank you for all my amazing mentors‚ù§Ô∏è:</strong></strong></p><p>Arun Edwin, Vikash, Sachin, Ebby, Vivek, Rehan, Vishwas, Nimmy, Vijay, Swamy, Kripa, Arthy, Sri, Hari.</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="https://labs.imaginea.com/content/images/2020/01/PSX_20200109_223113--1-.jpg" class="kg-image" alt="Snorkeling in Data for Supervision and Generation"><figcaption>Sachin's Send off [10/01/2020] | Shot on Vikash's OnePlus 7t ü§™</figcaption></figure><p>Originally Published at: <a href="https://labs.imaginea.com/snorkeling-in-data-for-supervision-and-generation/">Imaginea Labs</a></p><figure class="kg-card kg-image-card kg-width-full"><img src="https://labs.imaginea.com/content/images/2020/01/1_iGomvgpRqhDrvJaaxrtT2Q.png" class="kg-image" alt="Snorkeling in Data for Supervision and Generation"></figure>]]></content:encoded></item><item><title><![CDATA[Blue or Green Screen Effect with OpenCV [Chroma keying]]]></title><description><![CDATA[<p>Jump to <a href="https://github.com/tejakummarikuntla/blue-screen-effect-OpenCV/" rel="noopener nofollow">Code</a> with .ipynb</p><hr><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*PEELIgKtt9V_Q7heJFNy-g.png" class="kg-image"></figure><p>Before we get into Chroma keying[ green screen effect ] it‚Äôs better to understand the underlying concept that making it possible with Open CV.</p><h3 id="colour-thresholds">Colour Thresholds</h3><p>As we treat Images as grids of pixels as a function of X and Y, we are gonna use</p>]]></description><link>tejakummarikuntla.github.io/blog/blue-or-green-screen-effect-with-opencv-chroma-keying/</link><guid isPermaLink="false">5ead982e659dad2aec6708ca</guid><category><![CDATA[Image Processing]]></category><category><![CDATA[computer-vision]]></category><dc:creator><![CDATA[Teja Kummarikuntla]]></dc:creator><pubDate>Wed, 01 May 2019 16:00:00 GMT</pubDate><media:content url="tejakummarikuntla.github.io/blog/content/images/2020/05/1_PEELIgKtt9V_Q7heJFNy-g.png" medium="image"/><content:encoded><![CDATA[<img src="tejakummarikuntla.github.io/blog/content/images/2020/05/1_PEELIgKtt9V_Q7heJFNy-g.png" alt="Blue or Green Screen Effect with OpenCV [Chroma keying]"><p>Jump to <a href="https://github.com/tejakummarikuntla/blue-screen-effect-OpenCV/" rel="noopener nofollow">Code</a> with .ipynb</p><hr><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*PEELIgKtt9V_Q7heJFNy-g.png" class="kg-image" alt="Blue or Green Screen Effect with OpenCV [Chroma keying]"></figure><p>Before we get into Chroma keying[ green screen effect ] it‚Äôs better to understand the underlying concept that making it possible with Open CV.</p><h3 id="colour-thresholds">Colour Thresholds</h3><p>As we treat Images as grids of pixels as a function of X and Y, we are gonna use that information of colors to isolate a particular area.<code>selecting areas of intrest.</code> we‚Äôll be selecting an area of interest using Colour Thresholds,</p><p>With Colour Thresholds we can able to remove parts of an image that falls under a specific color range. The common use is with Blue/Green Screen.</p><p>A Blue Screen similar to a green screen is used to layer two images or video streams based on identifying and replacing a large blue area.</p><p>We‚Äôre gonna use Blue Screen to film now ;p. So, how does it work?</p><p>The first step is to isolate the blue background and replace that blue area with an image of your choosing.</p><hr><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/750/1*P7ypHvfCRifzVtg26EP5tA.jpeg" class="kg-image" alt="Blue or Green Screen Effect with OpenCV [Chroma keying]"><figcaption>source: <a href="https://www.youtube.com/channel/UCchJggclQ1aXyHI9kPrqhAg" data-href="https://www.youtube.com/channel/UCchJggclQ1aXyHI9kPrqhAg" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Crude Animation</a> (ctree_bluescreen.jpg)</figcaption></figure><p>We‚Äôll be starting with an image of a Christmas tree on a Blue screen background.</p><p>We first have to identify the blue region then later we‚Äôll replace it with a background image of our choosing.</p><hr><pre><code>import cv2
import matplotlib.pyplot as plt
import numpy as np
image = cv2.imread('images/ctree_bluescreen.jpg')</code></pre><p><code>cv2.imread()</code> is used to read an image which takes the image location as an argument, where the image <code>ctree_bluescreen.jpg</code> is in the folder called <code>images.</code></p><pre><code>print('Image type: ', type(image),
      'Image Dimensions : ', image.shape)</code></pre><p>This gives you the result as :</p><p><code>Image type:</code> &lt;class ‚Äònumpy.ndarray‚Äô&gt; <code>Image Dimesnsions:</code> (720, 1280, 3)</p><p>The openCV library reads the image as an array, also known as a grid or matrix of pixel values. The shape of the image, which contains three values that represent the dimensions of the image array,</p><p><code>720:</code> height in pixels</p><p><code>816:</code> width in pixels</p><p><code>3:</code> Colour Components for Red, Green and Blue (RGB) valuesimage_copy = np.copy(image)</p><pre><code>image_copy = np.copy(image)
image_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)
plt.imshow(image_copy)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/750/1*Xeyfrp-B2hgmV489VQR-4g.png" class="kg-image" alt="Blue or Green Screen Effect with OpenCV [Chroma keying]"><figcaption>Output for the above code&nbsp;snippet</figcaption></figure><p>Open CV reads in colour images as BGR(blue, green, red) images, not as RGB(red, blue, green). So, the Red and Blue colours are in reverse order and pyplot reflect this switch and results in a differently coloured image than original.</p><p>So, before we display the image let‚Äôs make a copy of the original image and use Open CV to change colour from BGR to RGB. It‚Äôs good practice to always make a copy of the image you‚Äôre working with. This way any transformation you‚Äôll apply to the copy will not affect the original image, so it‚Äôs easier to undo a step or try something new.</p><p>Now, on this copied image <code>image_copy</code> we can perform a colour transformation using Open CV function <code>cvtColor()</code> , this takes a source image and colour conversion code, in this case, it is just BGR2RGB and then outputs the desired image.</p><hr><h3 id="defining-the-colour-threshold">Defining the Colour Threshold</h3><p>Now, we need to create a colour threshold to remove the desired blue region.</p><p>To create a Colour Threshold, we need to define lower and upper bounds for the colour that we need to isolate and remove -blue</p><p>we‚Äôll be using the colour threshold values to eventually select the blue screen area that contains this range of colour values and get rid of it.</p><pre><code>lower_blue = np.array([0, 0, 100])     ##[R value, G value, B value]
upper_blue = np.array([120, 100, 255])</code></pre><p>So, we defined the low threshold that contains the lowest values of red, green and blue that are still considered part of the blue screen background.</p><p>In <code>lower_blue</code> , for red and green, we set as 0, meaning it‚Äôs okay to have no red or green. But, the lowest value for blue should still be quite high, let‚Äôs say around 100.</p><p>Now, for <code>upper_blue</code>defined the upper threshold to allow little more red and green, and set the highest value for blue to <code>255</code>. Any colour within this low and high range will be an intense blue colour. this is just an estimation though. So, if we find that this range isn‚Äôt finding the blue screen area we want, we can get back and change the values.</p><hr><h3 id="creating-a-mask">Creating a Mask</h3><p>We are gonna use the colour bound that are just created to create an image mask.</p><p>Masks are very common way to isolate a <code>selected area of intrest</code> and do something with that area. We can create a mask over blue area using Open CV‚Äôs <code>inRange()</code> function.</p><pre><code>mask = cv2.inRange(image_copy, lower_blue, upper_blue)
plt.imshow(mask, cmap='gray')</code></pre><p>The <code>inRange()</code> function takes in an image in our lower and upper colour bounds and defines a mask by asking if the colour value of each image pixel falls in the range of the lower and upper colour thresholds. If it does fall in this range, the mask will be allowed to be displayed and if not it will block it out and turn the pixel black.</p><p>In fact, we can visualize the mask by plotting it as we would an image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*mtSrH8zBZyTlSIPN7hGJYg.png" class="kg-image" alt="Blue or Green Screen Effect with OpenCV [Chroma keying]"><figcaption>plt.imshow(mask, cmap=‚Äôgray‚Äô)</figcaption></figure><p>The whole white area is where the image will be allowed to show through and the black will be blocked out. In numerical values, we can look at this mask as a 2D grid with the same dimensions as our image <code>720</code> pixels in height and <code>816</code> pixels in width.</p><p>Each coordinate in the mask has a value of either 255 for white and 0 for black, sort of like a grayscale image. And when we look at this mask we can see that it has a white area where the blue screen background is and the black area where the Christmas tree is.</p><p>Now, the first thing we need to do is let the Christmas tree show through and block the blue screen background.</p><hr><pre><code>masked_image = np.copy(image_copy)
masked_image[mask != 0] = [0, 0, 0]
plt.imshow(masked_image)</code></pre><p>First, to mask the image we are gonna make another image copy called <code>maksed_image</code> of our colour changed image copy, just in case I want to change the mask later on.</p><p>Then one way to select the blue screen is by asking for the part of that image that overlaps with the part of the mask that is white or not black. That is we‚Äôll select the part of the image where the area of the mask is not equal to zero, using <code>mask != 0</code> . And to block this background area out we then set the pixels to black. Now when we display our result, that should show the Christmas tree area is the only area that should show through.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*hsQ8XdFTqMmopzUxqEqDWw.png" class="kg-image" alt="Blue or Green Screen Effect with OpenCV [Chroma keying]"><figcaption>plt.imshow(masked_image)</figcaption></figure><p>The Blue screen background is gone, we might even change our colour threshold to get rid of any few blue spots, we can try it by increasing the highest green value and decreasing the low blue value, that should capture a larger range of blue.</p><hr><h3 id="mask-and-add-background-image">Mask and Add Background Image</h3><p>Now, we just have one last step which is to apply a background to this image. The process is fairly similar.</p><pre><code>background_image = cv2.imread('images/treeBackground.jpg')
background_image = cv2.cvtColor(background_image, cv2.COLOR_BGR2RGB)

crop_background = background_image[0:720, 0:1280]

crop_background[mask == 0] = [0, 0, 0]

plt.imshow(crop_background)</code></pre><p>First, we‚Äôll read in an image of outer space and convert into RGB Colour. We‚Äôll also crop it so that it‚Äôs the same size as our tree image <code>720 x 1280 pixels</code> , we are calling this image as <code>crop_background</code> , then we apply the mask, this time using the opposite mask, mean we want the background to show through and not the Christmas tree area. If we look back at the mak in this case we‚Äôre blocking the part of the background image where the mask is equal to zero.</p><p>Just to make sure we got this masking correct, we‚Äôre gonna plot the resulting image.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*rIxWdP6mUPRmuUgwwUT5mQ.png" class="kg-image" alt="Blue or Green Screen Effect with OpenCV [Chroma keying]"><figcaption>plt.imshow(crop_background)</figcaption></figure><p>The result is the background with the Tree cut out.</p><p>Then finally, we just need to add these two images together. Since the black area is equivalent to zeros in pixel colour value, a simple addition will work.</p><pre><code>final_image = crop_background + masked_image
plt.imshow(final_image)</code></pre><p>Now, when we plot the complete image I got the Christmas tree with new Background.üôå</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*1BT5fsCtfxh0n9mUvh7vhA.png" class="kg-image" alt="Blue or Green Screen Effect with OpenCV [Chroma keying]"><figcaption>plt.imshow(final_image)</figcaption></figure><p>Originally Published at: <a href="https://medium.com/fnplus/blue-or-green-screen-effect-with-open-cv-chroma-keying-94d4a6ab2743?source=---------2------------------">Medium</a></p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*iGomvgpRqhDrvJaaxrtT2Q.png" class="kg-image" alt="Blue or Green Screen Effect with OpenCV [Chroma keying]"></figure>]]></content:encoded></item><item><title><![CDATA[Camera Calibration with¬†OpenCV]]></title><description><![CDATA[<p>When we talk about camera calibration and Image distortion, we‚Äôre talking about what happens when a camera looks at 3D objects in the real world and transforms them into a 2D image. That transformation isn‚Äôt perfect.</p><p>For example, here‚Äôs an image of a road and some images</p>]]></description><link>tejakummarikuntla.github.io/blog/camera-calibration-with-opencv/</link><guid isPermaLink="false">5ead9647659dad2aec67089b</guid><category><![CDATA[computer-vision]]></category><dc:creator><![CDATA[Teja Kummarikuntla]]></dc:creator><pubDate>Sat, 09 Feb 2019 15:55:00 GMT</pubDate><media:content url="tejakummarikuntla.github.io/blog/content/images/2020/05/1_KR71E-iu_Z5PNCUATg6XIA.png" medium="image"/><content:encoded><![CDATA[<img src="tejakummarikuntla.github.io/blog/content/images/2020/05/1_KR71E-iu_Z5PNCUATg6XIA.png" alt="Camera Calibration with¬†OpenCV"><p>When we talk about camera calibration and Image distortion, we‚Äôre talking about what happens when a camera looks at 3D objects in the real world and transforms them into a 2D image. That transformation isn‚Äôt perfect.</p><p>For example, here‚Äôs an image of a road and some images taken through the different camera lens that slightly distorted.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*O41plkKxm9SFvpQEbt_U3A.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"><figcaption>An original picture of the&nbsp;road</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*BLv96PyejmWq1M5xjuKWUA.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"><figcaption>Distorted versions of the above picture by a&nbsp;camera</figcaption></figure><p>In these distorted images, you can see that the edges of the lanes are bent and sort of rounded or stretched outward. Our first step in analyzing camera is to undo this distortion so we can get correct and useful information out of them.</p><h3 id="why-distortion">Why Distortion?</h3><p>Before we get into the code and start correcting for distortion, let‚Äôs get some intuition as to how this distortion occurs.</p><p>Here‚Äôs a simple model of a camera called the pinhole camera model.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*7Wh9L_VYNsPbBf5m2u0aXw.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><p>When a camera looking at an object, it is looking at the world similar to how our eyes do. By focusing the light that‚Äôs reflected off of objects in the world. In this case, though a small pinhole, the camera focuses the light that‚Äôs reflected off to a 3D traffic sign and forms a 2D image at the back of the camera.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*R7Ec-32NFWWf9E7K9SgnRA.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><p>In math, the Transformation from 3D object points, P of X, Y and Z to X and Y is done by a transformative matrix called the<strong> camera matrix(C)</strong>, we‚Äôll be using this to calibrate the camera.</p><p>However, real cameras don‚Äôt use tiny pinholes; they use <strong>lenses</strong> to focus on multiple light rays at a time which allows them to quickly form images. But, lenses can introduce <strong>distortion</strong> too.</p><blockquote>Light lays often bend a little too much at the edges of a curved lens of a camera, and this creates the effect that distorts the edges of the images.</blockquote><h3 id="types-of-distortion">Types of Distortion</h3><p><strong><em>Radial Distortion:</em> </strong>Radial Distortion is the most common type that affects the images, In which when a camera captured pictures of straight lines appeared slightly curved or bent</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*fUl8POYptRmR4UzCM4Ox8Q.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"><figcaption>Radially Distorted by a&nbsp;camera</figcaption></figure><p><strong><em>Tangential distortion: </em></strong>Tangential distortion occurs mainly because the lens is not parallely aligned to the imaging plane, that makes the image to be extended a little while longer or tilted, it makes the objects appear farther away or even closer than they actually are.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/750/1*VaeeJWD3M3Qu2BbJ-hyuNg.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*bkF_vos8mQSFe7jy5JXsJA.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><p>So, In order to reduce the distortion, luckily this distortion can be captured by five numbers called <strong>Distortion Coefficients</strong>, whose values reflect the amount of radial and tangential distortion in an image.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*kDPV6S_yLNE15lwuC9nxQA.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><p>If we know the values of all the coefficients, we can use them to calibrate our camera and undistort the distorted images.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*2dc3TTGhlqfQNp_XJY3xKw.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"><figcaption>Undistorting the Distorted Image with Distortion Coefficients.</figcaption></figure><h3 id="measuring-distortion"><strong>Measuring Distortion</strong></h3><p>So, we know that the distortion changes the size and shape of the object in an image. But, how do we calibrate for that?</p><p>Well, we can take pictures of known shapes, then we‚Äôll be able to detect and correct any distortion errors. We could choose any shape to calibrate our camera, and we‚Äôll use a <strong>chessboard.</strong></p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/750/1*kuatnf9nLE0vZsC0qF7pTQ.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><p>A chessboard is great for calibration because it's regular, high contrast pattern makes it easy to detect automatically. And we know how an undistorted flat chessboard looks like. So, if we use our camera to take pictures of Chessboard at different angles</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*owW3QvK-yQqdVLxkDQn53g.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><h3 id="finding-corners">Finding Corners</h3><p>Open CV helps to automatically detect the corners and draw on it by <em>findChessboardCorners() </em>and<em> drawChessboardCorners()</em></p><p>Applying both functions to a sample image, results:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*hoHEjJmK58OaO0xmnItayg.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"><figcaption>After applying <em class="markup--em markup--figure-em">findChessboardCorners() </em>and<em class="markup--em markup--figure-em"> drawChessboardCorners()</em></figcaption></figure><pre><code>import numpy as np
import cv2
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# prepare object points
nx = 8 number of inside corners in x
ny = 6 number of inside corners in y
# Make a list of calibration images
fname = 'calibration_test.png'
img = cv2.imread(fname)
# Convert to grayscale
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# Find the chessboard corners
ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)
# If found, draw corners
if ret == True:
    # Draw and display the corners
    cv2.drawChessboardCorners(img, (nx, ny), corners, ret)
    plt.imshow(img)</code></pre><h3 id="calibrating-the-camera">Calibrating The Camera</h3><p>In order to Calibrate the camera, the first step will be to read in calibration Images of a chess board. It‚Äôs recommended to use at least 20 images to get a reliable calibration, For this, we have a lot of images here, each chess board has eight by six corners to detect,</p><p>To calibrate a camera, OpenCV gives us the <strong><em>calibrateCamera()</em></strong> function</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*-CDYzCBsuSB0uFN9GTd6yw.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><p>This takes in Object points, Image points[<em>will understand these points in a moment</em>], and the shape of the image and using these inputs, it calculates and returns</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*VVKMMo_RnI0bUw_6h4CXMA.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><p><strong>mtx</strong>: Camera Matrix, which helps to transform 3D objects points to 2D image points.</p><p><strong>dist:</strong> distortion coefficient</p><p>It also returns the position of the camera in the world, with the values of rotation and translation vectors <strong>rvecs, tvecs</strong></p><p>The next function that we require is <strong><em>undistort()</em></strong>.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*4wjojfc9y4edjrBW_Va0Lw.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><p>The undistort function takes in a distorted image, our camera matrix, and distortion coefficients and it returns an <strong>undistorted</strong>, often called <strong>destination image.</strong></p><p>In calibrateCamera() function we need object points and image points.</p><pre><code>import numpy as np 
import cv2
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
#read in a calibration image
img = mpimg.imread('../calibration_images/calibration1.jpg')
plt.imshow(img)</code></pre><p>First, Done with numpy, openCV, and plotting imports, then we are gonna read the first image <code>calibarion1.jpg</code> and display it.</p><p>Now, we are gonna map the coordinates of the corners in the 2D displayed image which called as <code>imagepoints</code> , to the 3D coordinates of the real, undistorted chessboard corners, which are called as <code>objectpoinst</code>.</p><p>So, we are gonna set up two empty arrays to hold these points, <code>objectpoints</code> and <code>imagepoints</code></p><pre><code># Arrays to store object points and image points from all the images
objpoints = [] # 3D points in real world space
imgpoints = [] # 2D points in image plane</code></pre><p>The object points will all be the same, just the known object corners of the chess board corners for an eight by six board.</p><p>So, we are going to prepare these object points, first by creating six by eight points in an array, each with three columns for the x,y and z coordinates of each corner. We will then initialize all these to 0s using Numpy‚Äôs zeros function. The z coordinates will stay zero so leave that as it is but, for our first two columns x and y, use Numpy‚Äôs <code>mgrid</code> function to generate the coordinates that we want. <code>mgrid</code> returns the coordinate values for given grid size and shape those coordinates back into two columns, one for x and one for y:</p><pre><code># Prepare obj points, like (0, 0, 0), (1, 0, 0), (2, 0, 0)....., (7, 5, 0)
objp = np.zeros((6*8,3), np.float32)
objp[:,:,] =  mp.mgrid[0:8,0:6].T.reshape(-1,2) # x,y coordinates</code></pre><p>Next to create the <code>imagepoints</code>, we need to consider the distorted calibrated image and detect the corners of the board. OpenCV gives us an easy way to detect chessboard corners with a function called <code>findChessboardCorners()</code>, that returns the corners found in a grayscale image.</p><p>So, we will convert the image to greyscale and then pass that to the <code>findChessboardCorners()</code> function. This function takes in a <code>grayscle</code> image along with the dimensions of the chess board corners. In this case 8 by 6 and last parameter is for any flags; there are none in this example:</p><pre><code># Convert image to grayscale
gray = cv2.cvtColor(img, cv2.COLOR_BRG2GRAY)
# Find the Chesse board corners
rer, corners = cv2.findChessboardCorners(gray, (8,6), None)</code></pre><p>If this function detects corners, we are gonna append those points to the image points array and also add our prepared object points <code>objp</code> to the <code>objectpoints</code> array. These object points will be the same for all of the calibration images since they represent a real chessboard.</p><pre><code># If corners are found, add object points, image points
if ret == True:
    imgpoints.append(corners)
    objpoints.append(objp)</code></pre><p>Next, we also draw the detected corners, with a call to <code>drawChessboardCorners()</code> , that takes in our image, corner dimensions and corner points.</p><pre><code># If corners are found, add object points, image points
if ret == True:
    imgpoints.append(corners)
    objpoints.append(objp)
    
    # Draw and display the corners
    img = cv2.drawChessboardCorners(img, (8,6), corners, ret)
    plt.imshow(img)</code></pre><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*7ofE5NjiT0kqLv3YnFMriQ.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><h3 id="correction-for-distortion">Correction for Distortion</h3><pre><code>import pickle
import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# Read in the saved objpoints and imgpoints
dist_pickle = pickle.load( open( "wide_dist_pickle.p", "rb" ) )
objpoints = dist_pickle["objpoints"]
imgpoints = dist_pickle["imgpoints"]
# Read in an image
img = cv2.imread('test_image.png')
def cal_undistort(img, objpoints, imgpoints):
    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[1:], None, None)
    undist = cv2.undistort(img, mtx, dist, None, mtx)
    return undist
undistorted = cal_undistort(img, objpoints, imgpoints)
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))
f.tight_layout()
ax1.imshow(img)
ax1.set_title('Original Image', fontsize=50)
ax2.imshow(undistorted)
ax2.set_title('Undistorted Image', fontsize=50)
plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)</code></pre><p>Get <a href="https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/files/Advanced_Lane_Finding_Images/correct_for_distortion/wide_dist_pickle.p" rel="noopener">distortion pickle file</a> and <a href="https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/files/Advanced_Lane_Finding_Images/correct_for_distortion/test_image.png" rel="noopener">test image</a></p><p>Output result:</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*KR71E-iu_Z5PNCUATg6XIA.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure><p>Reference: Udacity Self Driving Car Engineer Nanodegree</p><p>Originally Published at: <a href="https://medium.com/analytics-vidhya/camera-calibration-with-opencv-f324679c6eb7?source=---------3------------------">Analytics Vidhya</a></p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*GVQD9AZADus5ilHq8mOxJg.png" class="kg-image" alt="Camera Calibration with¬†OpenCV"></figure>]]></content:encoded></item><item><title><![CDATA[Automate GitHub Issues status of your organization with Webhooks]]></title><description><![CDATA[<h3 id="introduction">Introduction</h3><p>Let‚Äôs suppose you are holding an organization which builds products and develop them, you are gonna use GitHub repositories(public/private) to keep all your code or resources safe and secure without getting messed up. In the time, you will be using an amazing feature of GitHub repos</p>]]></description><link>tejakummarikuntla.github.io/blog/automate-github-issues-status-of-your-organization-with-webhooks/</link><guid isPermaLink="false">5ea5ac26f432ef475c61ec91</guid><category><![CDATA[Github]]></category><dc:creator><![CDATA[Teja Kummarikuntla]]></dc:creator><pubDate>Sat, 15 Dec 2018 17:00:00 GMT</pubDate><media:content url="tejakummarikuntla.github.io/blog/content/images/2020/04/1_-cOady3KKCe8TLcbBic13Q-1.png" medium="image"/><content:encoded><![CDATA[<h3 id="introduction">Introduction</h3><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/1_-cOady3KKCe8TLcbBic13Q-1.png" alt="Automate GitHub Issues status of your organization with Webhooks"><p>Let‚Äôs suppose you are holding an organization which builds products and develop them, you are gonna use GitHub repositories(public/private) to keep all your code or resources safe and secure without getting messed up. In the time, you will be using an amazing feature of GitHub repos to raise an Issue and look forward to someone in or out of the organization to solve it!</p><p>For now, your issue status is <strong><strong>open</strong></strong></p><figure class="kg-card kg-image-card"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/1_rWBx6xaarIg06_lSsgj87A.png" class="kg-image" alt="Automate GitHub Issues status of your organization with Webhooks"></figure><p>As the issue is open, you wanted to get updated to your database if it‚Äôs got commented or solved by someone. For this, the old traditional approach to keep requesting the Github server for status updates and then updating in your database alongside, which is not an effective approach and here comes the helping hand of Github feature ‚Äò<strong><strong>WebHooks</strong></strong>‚Äô which make the process easier</p><h3 id="webhooks">WebHooks</h3><figure class="kg-card kg-image-card"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/1_-cOady3KKCe8TLcbBic13Q.png" class="kg-image" alt="Automate GitHub Issues status of your organization with Webhooks"></figure><p>WebHooks is an amazing external service in which the server reach you out with a POST request automatically when certain events happen, rather continuously requesting the server for any changes. Learn more at <a href="https://developer.github.com/webhooks/" rel="noopener nofollow">Webhooks Guide</a>.</p><p><em><em>How do WebHooks works?</em></em></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/1_a2To5IKlXR-MZO_wM4rDhQ.png" class="kg-image" alt="Automate GitHub Issues status of your organization with Webhooks"><figcaption>GitHub WebHook sending the data to your server as <strong><strong>Issue</strong></strong> <strong><strong>Comment Created on a repo</strong></strong></figcaption></figure><h2 id="how-to-create-a-github-webhook">How to create a GitHub Webhook</h2><p>First and for most, Webhooks can only be created for an organization that you are holding.</p><p>Go to the settings page of your organization and look for the option <strong><strong>Webhooks</strong></strong></p><figure class="kg-card kg-image-card"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/1_3GpMCUNI3hleRnG_197rJg.png" class="kg-image" alt="Automate GitHub Issues status of your organization with Webhooks"></figure><p>The above image may change for times, but the configuration should be similar.</p><p><strong><strong>Payload URL</strong></strong></p><p>The URL you provide will be get triggered when particular events(<em><em>you could choose the</em></em><strong><strong><em><em> </em></em></strong></strong><em><em>required events in the third option ‚Äò</em></em><strong><strong><em><em>which Events would you like to trigger this webhook?</em></em></strong></strong><em><em>‚Äô)</em></em>happen</p><p>Payload URL should consist a service that should receive data(payload) from GitHub. <em><em>Follow up my next writings to learn how to write a service and deploy to Heroku that receives data</em></em><strong><strong>.</strong></strong></p><p><strong><strong>Content-type</strong></strong></p><p>The Content-type is a header, which is used to indicate the media type of the response. As GitHub Webhook sends the response in JSON format, we are gonna select ‚Äò<em><em>application/json‚Äô </em></em>Learn more at <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type" rel="noopener nofollow">MDN-ContentType</a></p><p><strong><strong>Secret</strong></strong></p><p>As you configure the payload link and content-type, here‚Äôs the security part to secure your service that receives data. This ‚Äòsecret‚Äô part is to limit the requests to those coming from GitHub, there are a couple of ways to do this. But, for simplicity, I‚Äôll be leaving it blank for now. Learn more about this at <a href="https://developer.github.com/webhooks/securing/" rel="noopener nofollow">Securing your WebHook</a>.</p><p><strong><strong>which Events would you like to trigger this webhook?</strong></strong></p><p>As it sounds, this option helps you to subscribe for particular events, In which the Payload URL should get triggered,</p><p>We are gonna select ‚Äú<em><em>Let me select Individual Events</em></em>‚Äù</p><figure class="kg-card kg-image-card"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/1_v_UbPpTEyPujKk0b15wSNg.png" class="kg-image" alt="Automate GitHub Issues status of your organization with Webhooks"></figure><p>You can choose your required Events, for getting issue status and issue comments we choose ‚ÄúIssue Comments‚Äù and ‚ÄúIssues‚Äù</p><figure class="kg-card kg-image-card"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/1_o330NIFMHCRONBXVkUujdQ.png" class="kg-image" alt="Automate GitHub Issues status of your organization with Webhooks"></figure><p>Click on Add webhook. you are ready to go!</p><p>To know the Recent Deliveries of Webhook, go back to your webhook page and select the existing WebHook and scroll all the way to down find<strong><strong> Recent Deliveries</strong></strong></p><figure class="kg-card kg-image-card"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/1_RUUZUQR7ylC9dHATy7cByg.png" class="kg-image" alt="Automate GitHub Issues status of your organization with Webhooks"></figure><p>Originally Published at: <a href="https://medium.com/fnplus/automate-github-issues-status-of-your-organization-with-webhooks-ec90e262aeef">FnPlus Tech Blog</a></p><figure class="kg-card kg-image-card"><img src="tejakummarikuntla.github.io/blog/content/images/2020/04/1_GVQD9AZADus5ilHq8mOxJg.png" class="kg-image" alt="Automate GitHub Issues status of your organization with Webhooks"></figure>]]></content:encoded></item></channel></rss>